{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSxdqQDIVTGOf45DiunTvW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArfaKhalid/Generative-AI/blob/main/Google_PALM_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started"
      ],
      "metadata": {
        "id": "-r6P20DKNJfE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        },
        "id": "L2Z4xpA8NE0M",
        "outputId": "6f139a02-a21e-40b0-a241-1ccc1bc206c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-cloud-aiplatform==1.36.2\n",
            "  Downloading google_cloud_aiplatform-1.36.2-py2.py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.36.2) (2.11.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.36.2) (1.23.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.36.2) (3.20.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.36.2) (23.2)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.36.2) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.36.2) (3.12.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.36.2) (1.11.0)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.36.2) (2.0.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.36.2) (1.62.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.36.2) (2.17.3)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.36.2) (2.31.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.36.2) (1.60.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.36.2) (1.48.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.36.2) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.36.2) (2.7.0)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.36.2) (2.8.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform==1.36.2) (0.13.0)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.10/dist-packages (from shapely<3.0.0dev->google-cloud-aiplatform==1.36.2) (1.23.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.36.2) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.36.2) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.36.2) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.36.2) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.36.2) (1.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.36.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.36.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.36.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.36.2) (2023.11.17)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.36.2) (0.5.1)\n",
            "Installing collected packages: google-cloud-aiplatform\n",
            "\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed google-cloud-aiplatform-1.36.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Install Vertex AI SDK\n",
        "!pip install google-cloud-aiplatform==1.36.2 --upgrade --user"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "Kjupl-RyNpKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticating notebook environment\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ],
      "metadata": {
        "id": "KTP7TS0nNsoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vertex AI PaLM API models\n",
        "\n",
        "The Vertex AI PaLM API enables you to test, customize, and deploy instances of Google’s large language models (LLM) called as PaLM,  so that you can leverage the capabilities of PaLM in your applications.\n",
        "\n",
        "### Model naming scheme\n",
        "Foundation model names have three components: use case, model size, and version number. The naming convention is in the format:  \n",
        "`<use case>-<model size>@<version number>`\n",
        "\n",
        "For example, text-bison@001 represents the Bison text model, version 001.\n",
        "\n",
        "The model sizes are as follows:\n",
        "- **Bison**: The best value in terms of capability and cost.\n",
        "- **Gecko**: The smallest and cheapest model for simple tasks.\n",
        "\n",
        "### Available models\n",
        "\n",
        "The Vertex AI PaLM API currently supports five models:\n",
        "\n",
        "*   `text-bison@001` : Fine-tuned to follow natural language instructions and is suitable for a variety of language tasks.\n",
        "*   `chat-bison@001` : Fine-tuned for multi-turn conversation use cases like building a chatbot.\n",
        "*   `textembedding-gecko@001` : Returns model embeddings for text inputs.\n",
        "\n",
        "* `code-bison@001`: A model fine-tuned to generate code based on a natural language description of the desired code. For example, it can generate a unit test for a function.\n",
        "\n",
        "* `code-gecko@001`: A model fine-tuned to suggest code completion based on the context in code that's written.\n",
        "\n",
        "* `codechat-bison@001`: A model fine-tuned for chatbot conversations that help with code-related questions.\n",
        "\n",
        "You can find more information about the properties of these [foundational models in the Generative AI Studio documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#foundation_models).\n"
      ],
      "metadata": {
        "id": "Tg4M3GMON6KS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "\n",
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "# Initialize Vertex AI SDK\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)"
      ],
      "metadata": {
        "id": "GRq2vTk8OJsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from IPython.display import Markdown, display\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from vertexai.language_models import (\n",
        "    TextGenerationModel,\n",
        "    TextEmbeddingModel,\n",
        "    ChatModel,\n",
        "    InputOutputTextPair,\n",
        "    CodeGenerationModel,\n",
        "    CodeChatModel,\n",
        ")"
      ],
      "metadata": {
        "id": "MMHIgUh9ONmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text generation with `text-bison@001`\n",
        "\n",
        "The text generation model from PaLM API that you will use in this notebook is `text-bison@001`.\n",
        "It is fine-tuned to follow natural language instructions and is suitable for a variety of language tasks, such as:\n",
        "\n",
        "- Classification\n",
        "- Sentiment analysis\n",
        "- Entity extraction\n",
        "- Extractive question-answering\n",
        "- Summarization\n",
        "- Re-writing text in a different style\n",
        "- Ad copy generation\n",
        "- Concept ideation\n",
        "- Concept simplification"
      ],
      "metadata": {
        "id": "5_PG4e_3ORt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model\n",
        "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
      ],
      "metadata": {
        "id": "Z9HRuDa6OWhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prompt design\n",
        "Prompt design is the process of creating prompts that elicit the desired response from a language model. Prompt design is an important part of using language models because it allows non-specialists to control the output of the model with minimal overhead. By carefully crafting the prompts, you can nudge the model to generate a desired result. Prompt design can be an efficient way to experiment with adapting an LLM for a specific use case. The iterative process of repeatedly updating prompts and assessing the model’s responses is sometimes called prompt engineering."
      ],
      "metadata": {
        "id": "hoYcf_bFOcB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hello PaLM\n",
        "Create your first prompt and send it to the text generation model."
      ],
      "metadata": {
        "id": "7uNyaWKxNH9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is a large language model?\"\n",
        "\n",
        "response = generation_model.predict(prompt=prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "ivmpkufsOnQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try out your prompt\n",
        "- What are the biggest challenges facing the healthcare industry?\n",
        "- What are the latest developments in the automotive industry?\n",
        "- What are the biggest opportunities in the retail industry?\n",
        "- (Try your own prompts!)"
      ],
      "metadata": {
        "id": "FrK3rEjSOzuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Create a numbered list of 10 items. Each item in the list should be a trend in the tech industry.\n",
        "\n",
        "Each trend should be less than 5 words.\"\"\"  # try your own prompt\n",
        "\n",
        "response = generation_model.predict(prompt=prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "2DPHsP0EOpPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt templates\n",
        "Prompt templates are useful if you have found a good way to structure your prompt that you can re-use. This can be also be helpful in limiting the open-endedness of freeform prompts. There are many ways to implement prompt templates, and below is just one example using f-strings."
      ],
      "metadata": {
        "id": "5HcJpfTCO7Wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_industry = \"tech\"  # try changing this to a different industry\n",
        "\n",
        "response = generation_model.predict(\n",
        "    prompt=f\"\"\"Create a numbered list of 10 items. Each item in the list should\n",
        "    be a trend in the {my_industry} industry.\n",
        "\n",
        "    Each trend should be less than 5 words.\"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "utRfxGbqPBXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model parameters for `text-bison@001`\n",
        "You can customize how the PaLM API behaves in response to your prompt by using the following parameters for `text-bison@001`:\n",
        "\n",
        " - `temperature`: higher means more \"creative\" responses\n",
        " - `max_output_tokens`: sets the max number of tokens in the output\n",
        " - `top_p`: higher means it will pull from more possible next tokens, based on cumulative probability\n",
        " - `top_k`: higher means it will sample from more possible next tokens\n",
        "\n",
        "The section below covers each parameter and how to use them.\n",
        "#### The `temperature` parameter (range: 0.0 - 1.0, default 0)\n",
        "\n",
        "##### What is _temperature_?\n",
        "The temperature is used for sampling during the response generation, which occurs when top_p and top_k are applied. Temperature controls the degree of randomness in token selection.\n",
        "\n",
        "##### How does _temperature_ affect the response?\n",
        "Lower temperatures are good for prompts that require a more deterministic and less open-ended response. In comparison, higher temperatures can lead to more \"creative\" or diverse results. A temperature of `0` is deterministic: the highest probability response is always selected. For most use cases, try starting with a temperature of `0.2`.\n",
        "\n",
        "A higher temperature value will result in a more exploratative output, with a higher likelihood of generating rare or unusual words or phrases. Conversely, a lower temperature value will result in a more conservative output, with a higher likelihood of generating common or expected words or phrases.\n",
        "\n",
        "##### Example:\n",
        "\n",
        "For example,\n",
        "\n",
        "`temperature = 0.0`:\n",
        "\n",
        "* _The cat sat on the couch, watching the birds outside._\n",
        "* _The cat sat on the windowsill, basking in the sun._\n",
        "\n",
        "`temperature = 0.9`:\n",
        "\n",
        "* _The cat sat on the moon, meowing at the stars._\n",
        "* _The cat sat on the cheeseburger, purring with delight._\n",
        "\n",
        "**Note**: It's important to note that while the temperature parameter can help generate more diverse and interesting text, it can also increase the likelihood of generating nonsensical or inappropriate text (i.e. hallucinations). Therefore, it's important to use it carefully and with consideration for the desired outcome.\n",
        "\n",
        "For more information on the `temperature` parameter for text models, please refer to the [documentation on model parameters](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#text_model_parameters).\n",
        "\n",
        "Note : If you run the following cell multiple times, it should always return the same response, as `temperature=0` is deterministic."
      ],
      "metadata": {
        "id": "4JaZrmhJPH_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp_val = 0.0\n",
        "prompt_temperature = \"Complete the sentence: As I prepared the picture frame, I reached into my toolkit to fetch my:\"\n",
        "\n",
        "response = generation_model.predict(\n",
        "    prompt=prompt_temperature,\n",
        "    temperature=temp_val,\n",
        ")\n",
        "\n",
        "print(f\"[temperature = {temp_val}]\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "FGbE7KD4PD5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_val = 1.0\n",
        "\n",
        "response = generation_model.predict(\n",
        "    prompt=prompt_temperature,\n",
        "    temperature=temp_val,\n",
        ")\n",
        "\n",
        "print(f\"[temperature = {temp_val}]\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "GvxaNwB7PZim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The `max_output_tokens` parameter (range: 1 - 1024, default 128)\n",
        "\n",
        "##### Tokens\n",
        "A single token may be smaller than a word. For example, a token is approximately four characters. So 100 tokens correspond to roughly 60-80 words. It's essential to be aware of the token sizes as models have a limit on input and output tokens.\n",
        "\n",
        "##### What is _max_output_tokens_?\n",
        "`max_output_tokens` is the maximum number of tokens that can be generated in the response.\n",
        "\n",
        "##### How does _max_output_tokens_ affect the response?\n",
        "\n",
        "Specify a lower value for shorter responses and a higher value for longer responses. A token may be smaller than a word. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words.\n",
        "\n",
        "For more information on the `max_output_tokens` parameter for text models, please refer to the [documentation on model parameters](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#text_model_parameters)."
      ],
      "metadata": {
        "id": "FnE0zbSbPi-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_output_tokens_val = 5\n",
        "\n",
        "response = generation_model.predict(\n",
        "    prompt=\"List ten ways that generative AI can help improve the online shopping experience for users\",\n",
        "    max_output_tokens=max_output_tokens_val,\n",
        ")\n",
        "\n",
        "print(f\"[max_output_tokens = {max_output_tokens_val}]\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "SgiVtLxlPexD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_output_tokens_val = 500\n",
        "\n",
        "response = generation_model.predict(\n",
        "    prompt=\"List ten ways that generative AI can help improve the online shopping experience for users\",\n",
        "    max_output_tokens=max_output_tokens_val,\n",
        ")\n",
        "\n",
        "print(f\"[max_output_tokens = {max_output_tokens_val}]\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "5h00UHo4Pn3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "ZdmWBLOkPuW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The `top_p` parameter (range: 0.0 - 1.0, default 0.95)\n",
        "​\n",
        "##### What is _top_p_?\n",
        "`top_p` controls how the model selects tokens for output by adjusting the probability distribution of the next word in the generated text based on a cumulative probability cutoff. Specifically, it selects the smallest set of tokens whose cumulative probability exceeds the given cutoff probability _p_, and samples from this set uniformly.\n",
        "​\n",
        "​\n",
        "For example, suppose tokens A, B, and C have a probability of 0.3, 0.2, and 0.1, and the `top_p` value is 0.5. In that case, the model will select either A or B as the next token (using temperature) and not consider C, because the cumulative probability of top_p is <= 0.5. Specify a lower value for less random responses and a higher value for more random responses.\n",
        "​\n",
        "##### How does _top_p_ affect the response?\n",
        "​\n",
        "The `top_p` parameter is used to control the diversity of the generated text. A higher `top_p` parameter value results in more \"diverse\" and \"interesting\" outputs, with the model being allowed to sample from a larger pool of possibilities. In contrast, a lower `top_p` parameter value resulted in more predictable outputs, with the model being constrained to a smaller set of possible tokens.\n",
        "​\n",
        "​\n",
        "##### Example:\n",
        "​\n",
        "`top_p = 0.1`:\n",
        "​\n",
        "- The cat sat on the mat.\n",
        "- The cat sat on the floor.\n",
        "​\n",
        "`top_p = 0.9`:\n",
        "​\n",
        "- The cat sat on the windowsill, soaking up the sun's rays.\n",
        "- The cat sat on the edge of the bed, watching the birds outside.\n",
        "​\n",
        "For more information on the `top_p` parameter for text models, please refer to the [documentation on model parameters](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#text_model_parameters)."
      ],
      "metadata": {
        "id": "S3NGye_ZPwr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_p_val = 0.0\n",
        "prompt_top_p_example = (\n",
        "    \"Create a marketing campaign for jackets that involves blue elephants and avocados.\"\n",
        ")\n",
        "\n",
        "response = generation_model.predict(\n",
        "    prompt=prompt_top_p_example, temperature=0.9, top_p=top_p_val\n",
        ")\n",
        "\n",
        "print(f\"[top_p = {top_p_val}]\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "tktJ0w2iP0Iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_p_val = 1.0\n",
        "\n",
        "response = generation_model.predict(\n",
        "    prompt=prompt_top_p_example, temperature=0.9, top_p=top_p_val\n",
        ")\n",
        "\n",
        "print(f\"[top_p = {top_p_val}]\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "rj2b_l74P5DA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The `top_k` parameter (range: 0.0 - 40, default 40)\n",
        "\n",
        "##### What is _top_k_?\n",
        "`top_k` changes how the model selects tokens for output. A `top_k` of 1 means the selected token is the most probable among all tokens in the model's vocabulary (also called greedy decoding). In contrast, a `top_k` of 3 means that the next token is selected from the top 3 most probable tokens (using temperature). For each token selection step, the `top_k` tokens with the highest probabilities are sampled. Then tokens are further filtered based on `top_p` with the final token selected using temperature sampling.\n",
        "\n",
        "##### How does _top_k_ affect the response?\n",
        "\n",
        "Specify a lower value for less random responses and a higher value for more random responses.\n",
        "\n",
        "For more information on the `top_k` parameter for text models, please refer to the [documentation on model parameters](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#text_model_parameters)."
      ],
      "metadata": {
        "id": "9yxdifx_P_5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_top_k_example = \"Write a 2-day itinerary for France.\"\n",
        "top_k_val = 1\n",
        "\n",
        "response = generation_model.predict(\n",
        "    prompt=prompt_top_k_example, max_output_tokens=300, temperature=0.9, top_k=top_k_val\n",
        ")\n",
        "\n",
        "print(f\"[top_k = {top_k_val}]\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "2dQ8nwMxP7Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_k_val = 40\n",
        "\n",
        "response = generation_model.predict(\n",
        "    prompt=prompt_top_k_example,\n",
        "    max_output_tokens=300,\n",
        "    temperature=0.9,\n",
        "    top_k=top_k_val,\n",
        ")\n",
        "\n",
        "print(f\"[top_k = {top_k_val}]\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "FY6BgtEkQIio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Chat model with chat-bison@001\n",
        "The `chat-bison@001` model lets you have a freeform conversation across multiple turns. The application tracks what was previously said in the conversation. As such, if you expect to use conversations in your application, use the `chat-bison@001` model because it has been fine-tuned for multi-turn conversation use cases."
      ],
      "metadata": {
        "id": "897P4LDpQLm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model = ChatModel.from_pretrained(\"chat-bison@001\")\n",
        "\n",
        "chat = chat_model.start_chat()\n",
        "\n",
        "print(\n",
        "    chat.send_message(\n",
        "        \"\"\"\n",
        "Hello! Can you write a 300 word abstract for a research paper I need to write about the impact of AI on society?\n",
        "\"\"\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "yUFEWGW8QJXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    chat.send_message(\n",
        "        \"\"\"\n",
        "Could you give me a catchy title for the paper?\n",
        "\"\"\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "38m81MQbQaFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advanced Chat model with the SDK\n",
        "You can also provide a `context` and `examples` to the model. The model will then respond based on the provided context and examples. You can also use `temperature`, `max_output_tokens`, `top_p`, and `top_k`. These parameters should be used when you start your chat with `chat_model.start_chat()`.\n",
        "\n",
        "For more information on chat models, please refer to the [documentation on chat model parameters](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#chat_model_parameters)."
      ],
      "metadata": {
        "id": "S9Qqu3s7Qd16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = chat_model.start_chat(\n",
        "    context=\"My name is Ned. You are my personal assistant. My favorite movies are Lord of the Rings and Hobbit.\",\n",
        "    examples=[\n",
        "        InputOutputTextPair(\n",
        "            input_text=\"Who do you work for?\",\n",
        "            output_text=\"I work for Ned.\",\n",
        "        ),\n",
        "        InputOutputTextPair(\n",
        "            input_text=\"What do I like?\",\n",
        "            output_text=\"Ned likes watching movies.\",\n",
        "        ),\n",
        "    ],\n",
        "    temperature=0.3,\n",
        "    max_output_tokens=200,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        ")\n",
        "print(chat.send_message(\"Are my favorite movies based on a book series?\"))"
      ],
      "metadata": {
        "id": "PTbh2mvbQhwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat.send_message(\"When where these books published?\"))"
      ],
      "metadata": {
        "id": "UsYaulUzQoEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding model with textembedding-gecko@001\n",
        "Text embeddings are a dense, often low-dimensional, vector representation of a piece of content such that, if two pieces of content are semantically similar, their respective embeddings are located near each other in the embedding vector space. This representation can be used to solve common NLP tasks, such as:\n",
        "\n",
        "* **Semantic search**: Search text ranked by semantic similarity.\n",
        "* **Recommendation**: Return items with text attributes similar to the given text.\n",
        "* **Classification**: Return the class of items whose text attributes are similar to the given text.\n",
        "* **Clustering**: Cluster items whose text attributes are similar to the given text.\n",
        "* **Outlier Detection**: Return items where text attributes are least related to the given text.\n",
        "\n",
        "Please refer to the [text embedding model documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings) for more information."
      ],
      "metadata": {
        "id": "V9lQ_uKuQp5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
        "\n",
        "embeddings = embedding_model.get_embeddings([\"What is life?\"])\n",
        "\n",
        "for embedding in embeddings:\n",
        "    vector = embedding.values\n",
        "    print(f\"Length = {len(vector)}\")\n",
        "    print(vector)"
      ],
      "metadata": {
        "id": "0hxRkUIzQykE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings and Pandas DataFrames\n",
        "If your text is stored in a column of a DataFrame, you can create a new column with the embeddings with the example below."
      ],
      "metadata": {
        "id": "OCjGaDoMQ3kx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\n",
        "    \"i really enjoyed the movie last night\",\n",
        "    \"so many amazing cinematic scenes yesterday\",\n",
        "    \"had a great time writing my Python scripts a few days ago\",\n",
        "    \"huge sense of relief when my .py script finally ran without error\",\n",
        "    \"O Romeo, Romeo, wherefore art thou Romeo?\",\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(text, columns=[\"text\"])\n",
        "df"
      ],
      "metadata": {
        "id": "hcHAvcWVRDCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "arMszOsqRGl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"embeddings\"] = df.apply(\n",
        "    lambda x: embedding_model.get_embeddings([x.text])[0].values, axis=1\n",
        ")\n",
        "df"
      ],
      "metadata": {
        "id": "CbkM6NmuRJ2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comparing similarity of text examples using cosine similarity\n",
        "By converting text into embeddings, you can compute similarity scores. There are many ways to compute similarity scores, and one common technique is using [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity).\n",
        "\n",
        "In the example from above, two of the sentences in the `text` column relate to enjoying a _movie_, and the other two relates to enjoying _coding_. Cosine similarity scores should be higher (closer to 1.0) when doing pairwise comparisons between semantically-related sentences, and scores should be lower between semantically-different sentences.\n",
        "\n",
        "The DataFrame output below shows the resulting cosine similarity scores between the embeddings:"
      ],
      "metadata": {
        "id": "5F7m0CcEROYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cos_sim_array = cosine_similarity(list(df.embeddings.values))\n",
        "\n",
        "# display as DataFrame\n",
        "df = pd.DataFrame(cos_sim_array, index=text, columns=text)\n",
        "df"
      ],
      "metadata": {
        "id": "FY9mOGFJRSd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make this easier to understand, you can use a heatmap. Naturally, text is most similar when they are identical (score of 1.0). The next highest scores are when sentences are semantically similar. The lowest scores are when sentences are quite different in meaning."
      ],
      "metadata": {
        "id": "Jq5eL9DSRYP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.heatmap(df, annot=True, cmap=\"crest\")\n",
        "ax.xaxis.tick_top()\n",
        "ax.set_xticklabels(text, rotation=90)"
      ],
      "metadata": {
        "id": "3dHmCCl6RX2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code generation with code-bison@001\n",
        "The code generation model (Codey) from PaLM API that you will use in this notebook is code-bison@001. It is fine-tuned to follow natural language instructions to generate required code and is suitable for a variety of coding tasks, such as:\n",
        "\n",
        "- writing functions\n",
        "- writing classes\n",
        "- web-pages\n",
        "- unit tests\n",
        "- docstrings\n",
        "- code translations, and many more use-cases.\n",
        "\n",
        "Currently it supports the following languages:\n",
        "- C++\n",
        "- C#\n",
        "- Go\n",
        "- GoogleSQL\n",
        "- Java\n",
        "- JavaScript\n",
        "- Kotlin\n",
        "- PHP\n",
        "- Python\n",
        "- Ruby\n",
        "- Rust\n",
        "- Scala\n",
        "- Swift\n",
        "- TypeScript\n",
        "\n",
        "You can find our more details [here](https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-models-overview)."
      ],
      "metadata": {
        "id": "oOkYwDD0Reo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "code_generation_model = CodeGenerationModel.from_pretrained(\"code-bison@001\")"
      ],
      "metadata": {
        "id": "6G2msu4NRm46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model parameters for `code-bison@001`\n",
        "\n",
        "You can customize how the PaLM API code generation behaves in response to your prompt by using the following parameters for `code-bison@001`:\n",
        "\n",
        " - `prefix`: it represents the beginning of a piece of meaningful programming code or a natural language prompt that describes code to be generated.\n",
        " - `temperature`: higher means more \"creative\" code responses. range: (0.0 - 1.0, default 0).\n",
        " - `max_output_tokens`: sets the max number of tokens in the output. range: (1 - 2048, default 2048)"
      ],
      "metadata": {
        "id": "U6n_MEvXRvGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hello Codey"
      ],
      "metadata": {
        "id": "XZHwMn8iRzBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"write a python function to do binary search\"\n",
        "\n",
        "response = code_generation_model.predict(prefix=prefix)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "Q0lf7uhbRxrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try out with example prompt\n",
        "Some examples:\n",
        "* write Go program to extract ip addresses from the text file\n",
        "* write Java program that can extract pincodes from addresses\n",
        "* write a standard SQL function that strips all non-alphabet characters from the string and encodes it to utf-8"
      ],
      "metadata": {
        "id": "L-7RCn49R6ZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"\"\"write a python function named as \"calculate_cosine_similarity\" and three unit \\\n",
        "            tests where it takes two arguments \"vector1\" and \"vector2\". \\\n",
        "            It then uses numpy dot function to calculate the dot product of the two vectors. \\n\n",
        "          \"\"\"\n",
        "\n",
        "response = code_generation_model.predict(prefix=prefix, max_output_tokens=1024)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "wzphZ1n0R9-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt templates\n",
        "Prompt templates are useful if you have found a good way to structure your prompt that you can re-use. This can be also be helpful in limiting the open-endedness of freeform prompts. There are many ways to implement prompt templates, and below is just one example using f-strings. This way you can structure the prompts as per the expected funcationality of the code."
      ],
      "metadata": {
        "id": "4Hb805M6SHuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "language = \"C++ function\"\n",
        "file_format = \"json\"\n",
        "extract_info = \"names\"\n",
        "requirments = \"\"\"\n",
        "              - the name should be start with capital letters.\n",
        "              - There should be no duplicate names in the final list.\n",
        "              \"\"\"\n",
        "\n",
        "prefix = f\"\"\"Create a {language} to parse {file_format} and extract {extract_info} with the following requirements: {requirments}.\n",
        "              \"\"\"\n",
        "\n",
        "response = code_generation_model.predict(prefix=prefix, max_output_tokens=1024)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "wbWCm0jCSOdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code completion with `code-gecko@001`\n",
        "Code completion uses the code-gecko foundation model to generate and complete code based on code being written. `code-gecko` completes code that was recently typed by a user.\n",
        "\n",
        "To learn more about creating prompts for code completion, see [Create prompts for code completion.](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/code-completion#:~:text=code%20completion%2C%20see-,Create%20prompts%20for%20code%20completion,-.)"
      ],
      "metadata": {
        "id": "4iDALELTSS8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code completion API has few more parameters than code generation.\n",
        "\n",
        "* prefix: *required* : For code models, prefix represents the beginning of a piece of meaningful programming code or a natural language prompt that describes code to be generated.\n",
        "\n",
        "* suffix: *optional* : For code completion, suffix represents the end of a piece of meaningful programming code. The model attempts to fill in the code in between the prefix and suffix.\n",
        "\n",
        "* temperature:  *required* : Temperature controls the degree of randomness in token selection. Same as for other models. range: (0.0 - 1.0, default 0)\n",
        "\n",
        "* maxOutputTokens: *required* : Maximum number of tokens that can be generated in the response. **range: (1 - 64, default 64)**\n",
        "\n",
        "* stopSequences: *optional*  : Specifies a list of strings that tells the model to stop generating text if one of the strings is encountered in the response. The strings are case-sensitive."
      ],
      "metadata": {
        "id": "lu58e_8lSaIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "code_completion_model = CodeGenerationModel.from_pretrained(\"code-gecko@001\")"
      ],
      "metadata": {
        "id": "em_KXoe9Sa46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"\"\"\n",
        "          def find_x_in_string(string_s, x):\n",
        "         \"\"\"\n",
        "\n",
        "response = code_completion_model.predict(prefix=prefix, max_output_tokens=64)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "qEjHa9RTSflp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"\"\"\n",
        "         def reverse_string(s):\n",
        "            return s[::-1]\n",
        "         def test_empty_input_string()\n",
        "         \"\"\"\n",
        "\n",
        "response = code_completion_model.predict(prefix=prefix, max_output_tokens=64)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "dcUntjWPSiwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code chat with `codechat-bison@001`\n",
        "The `codechat-bison@001` model lets you have a freeform conversation across multiple turns from a code context. The application tracks what was previously said in the conversation. As such, if you expect to use conversations in your application for code generation, use the `codechat-bison@001` model because it has been fine-tuned for multi-turn conversation use cases."
      ],
      "metadata": {
        "id": "QfDymofUSlAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "code_chat_model = CodeChatModel.from_pretrained(\"codechat-bison@001\")\n",
        "\n",
        "code_chat = code_chat_model.start_chat()\n",
        "\n",
        "print(\n",
        "    code_chat.send_message(\n",
        "        \"Please help write a function to calculate the min of two numbers\",\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "UCOc0UEiSpCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can take another example and ask the model to give more general code suggestion for a specific problem that you are working on."
      ],
      "metadata": {
        "id": "w4kXmVQtSuKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "code_chat = code_chat_model.start_chat()\n",
        "\n",
        "print(\n",
        "    code_chat.send_message(\n",
        "        \"what is the most scalable way to traverse a list in python?\",\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "MVGQwdT-SwMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can continue to ask follow-up questions to the origianl query."
      ],
      "metadata": {
        "id": "6pAzkSzQS2Jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    code_chat.send_message(\n",
        "        \"how would i measure the iteration per second for the following code?\",\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "zy2QZhDpS4ap"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}